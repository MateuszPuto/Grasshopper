{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DifferentiableSort(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Differentiable Sort utility.\n",
    "    Input: raw attention scores (e.g., QK^T)\n",
    "    Output: a soft permutation matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, scores):\n",
    "        # We need to sort by importance.\n",
    "        # Let's say we want to sort each row of the scores matrix.\n",
    "        # A simple way is to use a softmax with a low temperature.\n",
    "        \n",
    "        # scores has shape (batch, query_seq_len, key_seq_len)\n",
    "        # We want to create a permutation for *each* query.\n",
    "        \n",
    "        # Sort scores to get the indices. This is non-differentiable.\n",
    "        # So we create an approximation.\n",
    "        \n",
    "        # A simple soft permutation matrix can be a softmax over the scores themselves\n",
    "        # with a low temperature.\n",
    "        # The lower the temperature, the closer it is to a hard sort (one-hot vectors).\n",
    "        \n",
    "        # This implementation is a simplified approach for demonstration.\n",
    "        # More advanced methods use Gumbel-Softmax or Sinkhorn networks.\n",
    "        soft_perm_matrix = F.softmax(scores / self.temperature, dim=-1)\n",
    "        \n",
    "        # The key idea here is that the matrix P represents P_ij:\n",
    "        # P_ij = soft probability that original item j is in the i-th sorted position.\n",
    "        # This is a bit of a simplification, but conceptually it works.\n",
    "        \n",
    "        return soft_perm_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1d419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming DifferentiableSort class from above is defined\n",
    "#\n",
    "# A custom Multi-Head Attention module that uses a \n",
    "# convolutional sweep over an attention-ordered sequence.\n",
    "class AttentionWithConvSweep(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, kernel_size=3, padding='same', temperature=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Ensure d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Q, K, V projections\n",
    "        self.query_projection = nn.Linear(d_model, d_model)\n",
    "        self.key_projection = nn.Linear(d_model, d_model)\n",
    "        self.value_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # The Differentiable Sort utility\n",
    "        self.differentiable_sort = DifferentiableSort(temperature=temperature)\n",
    "\n",
    "        # 1D Convolution layer\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.head_dim,\n",
    "            out_channels=self.head_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # Final linear layer to project back to d_model\n",
    "        self.final_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # 1. Project input to Q, K, V\n",
    "        Q = self.query_projection(x)\n",
    "        K = self.key_projection(x)\n",
    "        V = self.value_projection(x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2. Calculate Scaled Dot-Product Attention Scores\n",
    "        # Q and K have shape (batch, num_heads, seq_len, head_dim)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        # scores now has shape (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "        if mask is not None:    \n",
    "            scores = scores.masked_fill(mask == float('-inf'), float('-inf'))\n",
    "            \n",
    "        # 3. Apply Differentiable Sort to get a soft permutation matrix\n",
    "        # This is where your core idea comes to life.\n",
    "        # We are generating a permutation matrix for each attention head.\n",
    "        soft_perm_matrix = self.differentiable_sort(scores)\n",
    "        # soft_perm_matrix has shape (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # 4. Perform Soft Re-ordering of Value vectors\n",
    "        # V has shape (batch, num_heads, seq_len, head_dim)\n",
    "        V_sorted = torch.matmul(soft_perm_matrix, V)\n",
    "        # V_sorted now has the same shape, but the vectors are \"softly\" re-ordered\n",
    "\n",
    "        # 5. Apply 1D Convolution\n",
    "        # nn.Conv1d expects input of shape (batch, channels, length)\n",
    "        # So we need to reshape V_sorted.\n",
    "        V_conv_input = V_sorted.transpose(1, 2).reshape(\n",
    "            batch_size * seq_len, self.num_heads, self.head_dim\n",
    "        )\n",
    "        \n",
    "        # Apply the convolution to each head's data\n",
    "        # Transpose again to get (batch, channels, length) for convolution\n",
    "        V_conv_input = V_conv_input.transpose(1, 2)\n",
    "\n",
    "        # ##\n",
    "        # k = self.conv1d.kernel_size[0]\n",
    "        # V_conv_input = F.pad(V_conv_input, (k - 1, 0))   # (left, right) padding\n",
    "        \n",
    "        # The output of the convolution\n",
    "        conv_output = self.conv1d(V_conv_input)\n",
    "        \n",
    "        # Reshape back to multi-head format\n",
    "        conv_output = conv_output.transpose(1, 2)\n",
    "        conv_output = conv_output.view(\n",
    "            batch_size, seq_len, self.num_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        # 6. Final projection and reshape\n",
    "        # Concatenate heads\n",
    "        output = conv_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.final_projection(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b14612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = AttentionWithConvSweep(d_model, num_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention and Residual Connection\n",
    "        attention_output = self.attention(x, mask=mask)\n",
    "        x = self.norm1(x + self.dropout1(attention_output))\n",
    "        \n",
    "        # Feed-Forward and Residual Connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b27d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, ff_dim=2048, n_layers=6, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb   = nn.Embedding(max_len, d_model)\n",
    "        self.layers    = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, ff_dim)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, T = x.shape\n",
    "        positions = torch.arange(T, device=x.device).unsqueeze(0)\n",
    "        h = self.token_emb(x) + self.pos_emb(positions)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, mask=mask)\n",
    "        h = self.ln(h)\n",
    "        return self.head(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e28d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mputo/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mputo/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # or another LM tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_dataset(\"allenai/c4\", \"en\", streaming=True, split=\"train\")\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "def encode(ex):\n",
    "    ids = tokenizer(ex[\"text\"], truncation=True, max_length=max_len,\n",
    "                    padding=\"max_length\", return_tensors=\"pt\").input_ids[0]\n",
    "    return {\"input_ids\": ids}\n",
    "\n",
    "tokenized = dataset.map(encode, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3854e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "    # def collate(batch):\n",
    "    #     x = torch.stack([b[\"input_ids\"] for b in batch])\n",
    "    #     return x, x  # input and target are the same for next-token LM\n",
    "\n",
    "def collate(batch, pad_token_id=0):\n",
    "    # stack into (B, T)\n",
    "    x = torch.stack([b[\"input_ids\"] for b in batch])\n",
    "    # input is everything except final token\n",
    "    input_ids  = x[:, :-1]\n",
    "    # target is everything except first token\n",
    "    target_ids = x[:, 1:]\n",
    "    return input_ids, target_ids\n",
    "\n",
    "loader = DataLoader(tokenized, batch_size=8, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13bbd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask_inputs(x):\n",
    "    # x: (batch, seq_len, dim)\n",
    "    B, T, D = x.shape\n",
    "    # create upper-triangular mask: (T, T)\n",
    "    mask = torch.tril(torch.ones(T, T, device=x.device))\n",
    "    # expand to batch and feature dims\n",
    "    mask = mask.unsqueeze(0).unsqueeze(-1)          # (1, T, T, 1)\n",
    "    x = x.unsqueeze(1)                              # (B, 1, T, D)\n",
    "    # broadcast so position i only keeps past j â‰¤ i\n",
    "    x_masked = (x * mask).sum(dim=2)                # (B, T, D)\n",
    "    return x_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7a137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 10.9290\n",
      "step 100 | loss 3.1340\n",
      "step 200 | loss 5.6683\n",
      "step 300 | loss 4.1739\n",
      "step 400 | loss 4.3506\n",
      "step 500 | loss 4.7017\n",
      "step 600 | loss 2.9628\n",
      "step 700 | loss 3.1012\n",
      "step 800 | loss 2.6092\n",
      "step 900 | loss 2.2340\n",
      "step 1000 | loss 4.6275\n",
      "step 1100 | loss 4.1496\n",
      "step 1200 | loss 1.7028\n",
      "step 1300 | loss 3.3404\n",
      "step 1400 | loss 3.5240\n",
      "step 1500 | loss 4.5412\n",
      "step 1600 | loss 4.4101\n",
      "step 1700 | loss 3.7683\n",
      "step 1800 | loss 4.1143\n",
      "step 1900 | loss 4.3877\n",
      "step 2000 | loss 1.4420\n",
      "step 2100 | loss 4.2939\n",
      "step 2200 | loss 4.3631\n",
      "step 2300 | loss 6.1179\n",
      "step 2400 | loss 4.5910\n",
      "step 2500 | loss 4.2241\n",
      "step 2600 | loss 3.8288\n",
      "step 2700 | loss 4.3593\n",
      "step 2800 | loss 2.7222\n",
      "step 2900 | loss 2.5809\n",
      "step 3000 | loss 3.3163\n",
      "step 3100 | loss 3.5158\n",
      "step 3200 | loss 3.6414\n",
      "step 3300 | loss 3.1643\n",
      "step 3400 | loss 3.2544\n",
      "step 3500 | loss 3.0553\n",
      "step 3600 | loss 3.3687\n",
      "step 3700 | loss 2.6648\n",
      "step 3800 | loss 3.9459\n",
      "step 3900 | loss 3.5089\n",
      "step 4000 | loss 2.7996\n",
      "step 4100 | loss 3.0660\n",
      "step 4200 | loss 3.3697\n",
      "step 4300 | loss 2.0417\n",
      "step 4400 | loss 2.9764\n",
      "step 4500 | loss 3.7372\n",
      "step 4600 | loss 3.4194\n",
      "step 4700 | loss 3.0637\n",
      "step 4800 | loss 3.3237\n",
      "step 4900 | loss 2.9050\n",
      "step 5000 | loss 2.1695\n",
      "step 5100 | loss 3.0761\n",
      "step 5200 | loss 2.0618\n",
      "step 5300 | loss 4.5299\n",
      "step 5400 | loss 2.2308\n",
      "step 5500 | loss 4.2370\n",
      "step 5600 | loss 3.9253\n",
      "step 5700 | loss 3.8603\n",
      "step 5800 | loss 3.5099\n",
      "step 5900 | loss 2.0145\n",
      "step 6000 | loss 3.4858\n",
      "step 6100 | loss 3.3925\n",
      "step 6200 | loss 3.8149\n",
      "step 6300 | loss 3.1804\n",
      "step 6400 | loss 2.2657\n",
      "step 6500 | loss 3.7486\n",
      "step 6600 | loss 2.9597\n",
      "step 6700 | loss 2.1278\n",
      "step 6800 | loss 4.9770\n",
      "step 6900 | loss 4.1738\n",
      "step 7000 | loss 2.3017\n",
      "step 7100 | loss 3.3812\n",
      "step 7200 | loss 3.7101\n",
      "step 7300 | loss 3.0924\n",
      "step 7400 | loss 2.9564\n",
      "step 7500 | loss 3.5227\n",
      "step 7600 | loss 4.0176\n",
      "step 7700 | loss 3.1295\n",
      "step 7800 | loss 2.8018\n",
      "step 7900 | loss 3.4995\n",
      "step 8000 | loss 3.8257\n",
      "step 8100 | loss 2.7759\n",
      "step 8200 | loss 2.6822\n",
      "step 8300 | loss 2.6313\n",
      "step 8400 | loss 2.8709\n",
      "step 8500 | loss 2.4816\n",
      "step 8600 | loss 2.0667\n",
      "step 8700 | loss 3.2200\n",
      "step 8800 | loss 2.8990\n",
      "step 8900 | loss 3.7434\n",
      "step 9000 | loss 3.2909\n",
      "step 9100 | loss 3.2059\n",
      "step 9200 | loss 2.8261\n",
      "step 9300 | loss 3.3198\n",
      "step 9400 | loss 3.1821\n",
      "step 9500 | loss 2.5254\n",
      "step 9600 | loss 3.1664\n",
      "step 9700 | loss 3.1634\n",
      "step 9800 | loss 2.5150\n",
      "step 9900 | loss 2.4480\n",
      "step 10000 | loss 3.0149\n",
      "step 10100 | loss 3.4217\n",
      "step 10200 | loss 3.6559\n",
      "step 10300 | loss 3.1940\n",
      "step 10400 | loss 2.9680\n",
      "step 10500 | loss 3.4394\n",
      "step 10600 | loss 2.7691\n",
      "step 10700 | loss 5.1175\n",
      "step 10800 | loss 3.2116\n",
      "step 10900 | loss 2.7844\n",
      "step 11000 | loss 3.7542\n",
      "step 11100 | loss 1.5593\n",
      "step 11200 | loss 2.7712\n",
      "step 11300 | loss 2.3742\n",
      "step 11400 | loss 3.3147\n",
      "step 11500 | loss 3.3161\n",
      "step 11600 | loss 2.4619\n",
      "step 11700 | loss 2.5748\n",
      "step 11800 | loss 3.0602\n",
      "step 11900 | loss 3.8139\n",
      "step 12000 | loss 2.6809\n",
      "step 12100 | loss 2.8143\n",
      "step 12200 | loss 3.9068\n",
      "step 12300 | loss 2.7639\n",
      "step 12400 | loss 2.8956\n",
      "step 12500 | loss 3.1566\n",
      "step 12600 | loss 3.4704\n",
      "step 12700 | loss 3.2193\n",
      "step 12800 | loss 2.8197\n",
      "step 12900 | loss 3.0064\n",
      "step 13000 | loss 3.3759\n",
      "step 13100 | loss 3.4841\n",
      "step 13200 | loss 2.5591\n",
      "step 13300 | loss 3.7662\n",
      "step 13400 | loss 3.7090\n",
      "step 13500 | loss 3.3649\n",
      "step 13600 | loss 3.0590\n",
      "step 13700 | loss 3.8088\n",
      "step 13800 | loss 2.0647\n",
      "step 13900 | loss 3.0615\n",
      "step 14000 | loss 3.0727\n",
      "step 14100 | loss 2.8631\n",
      "step 14200 | loss 1.9731\n",
      "step 14300 | loss 3.3188\n",
      "step 14400 | loss 3.5741\n",
      "step 14500 | loss 2.9438\n",
      "step 14600 | loss 3.3627\n",
      "step 14700 | loss 3.5309\n",
      "step 14800 | loss 2.0987\n",
      "step 14900 | loss 2.4500\n",
      "step 15000 | loss 3.6814\n",
      "step 15100 | loss 3.1589\n",
      "step 15200 | loss 3.5865\n",
      "step 15300 | loss 2.3024\n",
      "step 15400 | loss 3.3118\n",
      "step 15500 | loss 1.9925\n",
      "step 15600 | loss 3.3021\n",
      "step 15700 | loss 3.4659\n",
      "step 15800 | loss 2.6675\n",
      "step 15900 | loss 3.4058\n",
      "step 16000 | loss 3.5094\n",
      "step 16100 | loss 3.2691\n",
      "step 16200 | loss 2.5663\n",
      "step 16300 | loss 2.3180\n",
      "step 16400 | loss 2.4514\n",
      "step 16500 | loss 3.0499\n",
      "step 16600 | loss 3.1372\n",
      "step 16700 | loss 3.1125\n",
      "step 16800 | loss 1.7449\n",
      "step 16900 | loss 2.3567\n",
      "step 17000 | loss 2.7660\n",
      "step 17100 | loss 2.1215\n",
      "step 17200 | loss 3.0546\n",
      "step 17300 | loss 2.2693\n",
      "step 17400 | loss 2.7816\n",
      "step 17500 | loss 2.8744\n",
      "step 17600 | loss 3.3073\n",
      "step 17700 | loss 2.6527\n",
      "step 17800 | loss 3.6371\n",
      "step 17900 | loss 1.7521\n",
      "step 18000 | loss 2.8735\n",
      "step 18100 | loss 3.2196\n",
      "step 18200 | loss 2.0181\n",
      "step 18300 | loss 3.3947\n",
      "step 18400 | loss 1.5350\n",
      "step 18500 | loss 3.0482\n",
      "step 18600 | loss 2.7354\n",
      "step 18700 | loss 2.2648\n",
      "step 18800 | loss 3.1839\n",
      "step 18900 | loss 2.8360\n",
      "step 19000 | loss 2.1891\n",
      "step 19100 | loss 3.2104\n",
      "step 19200 | loss 2.6891\n",
      "step 19300 | loss 2.9774\n",
      "step 19400 | loss 1.8559\n",
      "step 19500 | loss 2.6851\n",
      "step 19600 | loss 3.4246\n",
      "step 19700 | loss 2.7929\n",
      "step 19800 | loss 2.8725\n",
      "step 19900 | loss 3.1620\n",
      "step 20000 | loss 3.0705\n",
      "step 20100 | loss 3.0810\n",
      "step 20200 | loss 3.0098\n",
      "step 20300 | loss 2.8565\n",
      "step 20400 | loss 3.2981\n",
      "step 20500 | loss 2.3179\n",
      "step 20600 | loss 4.2160\n",
      "step 20700 | loss 1.5700\n",
      "step 20800 | loss 3.0175\n",
      "step 20900 | loss 2.6810\n",
      "step 21000 | loss 3.0433\n",
      "step 21100 | loss 4.0337\n",
      "step 21200 | loss 2.0655\n",
      "step 21300 | loss 2.8474\n",
      "step 21400 | loss 2.4837\n",
      "step 21500 | loss 3.4567\n",
      "step 21600 | loss 2.8488\n",
      "step 21700 | loss 2.8444\n",
      "step 21800 | loss 1.5480\n",
      "step 21900 | loss 3.5871\n",
      "step 22000 | loss 3.1200\n",
      "step 22100 | loss 1.3209\n",
      "step 22200 | loss 2.8120\n",
      "step 22300 | loss 3.4825\n",
      "step 22400 | loss 2.7085\n",
      "step 22500 | loss 3.3454\n",
      "step 22600 | loss 3.0493\n",
      "step 22700 | loss 3.9350\n",
      "step 22800 | loss 3.1816\n",
      "step 22900 | loss 2.6793\n",
      "step 23000 | loss 1.7137\n",
      "step 23100 | loss 2.2798\n",
      "step 23200 | loss 1.7511\n",
      "step 23300 | loss 2.3987\n",
      "step 23400 | loss 3.2185\n",
      "step 23500 | loss 2.5889\n",
      "step 23600 | loss 2.8240\n",
      "step 23700 | loss 2.6028\n",
      "step 23800 | loss 3.3356\n",
      "step 23900 | loss 3.6883\n",
      "step 24000 | loss 2.8866\n",
      "step 24100 | loss 2.1287\n",
      "step 24200 | loss 2.6523\n",
      "step 24300 | loss 3.0492\n",
      "step 24400 | loss 2.3936\n",
      "step 24500 | loss 4.1043\n",
      "step 24600 | loss 2.3889\n",
      "step 24700 | loss 2.3690\n",
      "step 24800 | loss 1.0945\n",
      "step 24900 | loss 2.3259\n",
      "step 25000 | loss 3.0442\n",
      "step 25100 | loss 2.7146\n",
      "step 25200 | loss 2.7024\n",
      "step 25300 | loss 3.3288\n",
      "step 25400 | loss 3.5313\n",
      "step 25500 | loss 2.6970\n",
      "step 25600 | loss 2.6538\n",
      "step 25700 | loss 2.8227\n",
      "step 25800 | loss 2.9108\n",
      "step 25900 | loss 2.7075\n",
      "step 26000 | loss 2.7840\n",
      "step 26100 | loss 3.0276\n",
      "step 26200 | loss 2.6502\n",
      "step 26300 | loss 2.5721\n",
      "step 26400 | loss 2.1396\n",
      "step 26500 | loss 2.9119\n",
      "step 26600 | loss 2.7690\n",
      "step 26700 | loss 2.9467\n",
      "step 26800 | loss 2.4855\n",
      "step 26900 | loss 3.2264\n",
      "step 27000 | loss 2.6715\n",
      "step 27100 | loss 3.2950\n",
      "step 27200 | loss 2.2567\n",
      "step 27300 | loss 2.5895\n",
      "step 27400 | loss 1.6534\n",
      "step 27500 | loss 1.9623\n",
      "step 27600 | loss 2.1361\n",
      "step 27700 | loss 2.6791\n",
      "step 27800 | loss 2.4547\n",
      "step 27900 | loss 2.7183\n",
      "step 28000 | loss 3.5006\n",
      "step 28100 | loss 2.8228\n",
      "step 28200 | loss 2.6057\n",
      "step 28300 | loss 2.6622\n",
      "step 28400 | loss 2.2324\n",
      "step 28500 | loss 3.0500\n",
      "step 28600 | loss 1.6912\n",
      "step 28700 | loss 2.8833\n",
      "step 28800 | loss 1.9954\n",
      "step 28900 | loss 2.6550\n",
      "step 29000 | loss 3.1301\n",
      "step 29100 | loss 3.0600\n",
      "step 29200 | loss 2.3296\n",
      "step 29300 | loss 2.9234\n",
      "step 29400 | loss 3.5741\n",
      "step 29500 | loss 3.0968\n",
      "step 29600 | loss 2.9377\n",
      "step 29700 | loss 4.0479\n",
      "step 29800 | loss 2.5349\n",
      "step 29900 | loss 2.3677\n",
      "step 30000 | loss 3.4654\n",
      "step 30100 | loss 2.2771\n",
      "step 30200 | loss 2.2072\n",
      "step 30300 | loss 3.3612\n",
      "step 30400 | loss 3.3293\n",
      "step 30500 | loss 2.9087\n",
      "step 30600 | loss 2.4736\n",
      "step 30700 | loss 2.8884\n",
      "step 30800 | loss 2.9183\n",
      "step 30900 | loss 2.3174\n",
      "step 31000 | loss 2.0356\n",
      "step 31100 | loss 2.5494\n",
      "step 31200 | loss 2.4308\n",
      "step 31300 | loss 2.3065\n",
      "step 31400 | loss 2.1935\n",
      "step 31500 | loss 1.7886\n",
      "step 31600 | loss 2.8530\n",
      "step 31700 | loss 3.1868\n",
      "step 31800 | loss 2.9090\n",
      "step 31900 | loss 2.8189\n",
      "step 32000 | loss 3.3150\n",
      "step 32100 | loss 2.0784\n",
      "step 32200 | loss 2.3330\n",
      "step 32300 | loss 1.8460\n",
      "step 32400 | loss 2.8235\n",
      "step 32500 | loss 2.6414\n",
      "step 32600 | loss 2.8604\n",
      "step 32700 | loss 1.6763\n",
      "step 32800 | loss 3.1485\n",
      "step 32900 | loss 2.6422\n",
      "step 33000 | loss 3.3864\n",
      "step 33100 | loss 2.4090\n",
      "step 33200 | loss 3.0425\n",
      "step 33300 | loss 3.1001\n",
      "step 33400 | loss 3.3067\n",
      "step 33500 | loss 2.3635\n",
      "step 33600 | loss 3.0265\n",
      "step 33700 | loss 3.0033\n",
      "step 33800 | loss 3.0850\n",
      "step 33900 | loss 4.1266\n",
      "step 34000 | loss 2.2738\n",
      "step 34100 | loss 3.1778\n",
      "step 34200 | loss 1.5150\n",
      "step 34300 | loss 1.6915\n",
      "step 34400 | loss 1.5673\n",
      "step 34500 | loss 2.6740\n",
      "step 34600 | loss 2.2850\n",
      "step 34700 | loss 2.5136\n",
      "step 34800 | loss 1.7974\n",
      "step 34900 | loss 2.4035\n",
      "step 35000 | loss 2.7454\n",
      "step 35100 | loss 1.6917\n",
      "step 35200 | loss 2.9620\n",
      "step 35300 | loss 2.5003\n",
      "step 35400 | loss 3.1230\n",
      "step 35500 | loss 3.9322\n",
      "step 35600 | loss 2.8582\n",
      "step 35700 | loss 2.2105\n",
      "step 35800 | loss 2.2593\n",
      "step 35900 | loss 2.9854\n",
      "step 36000 | loss 2.3738\n",
      "step 36100 | loss 2.5080\n",
      "step 36200 | loss 2.5991\n",
      "step 36300 | loss 2.5371\n",
      "step 36400 | loss 3.1012\n",
      "step 36500 | loss 2.9895\n",
      "step 36600 | loss 3.2255\n",
      "step 36700 | loss 2.3075\n",
      "step 36800 | loss 2.2371\n",
      "step 36900 | loss 2.7055\n",
      "step 37000 | loss 2.9912\n",
      "step 37100 | loss 2.5433\n",
      "step 37200 | loss 1.7685\n",
      "step 37300 | loss 1.3631\n",
      "step 37400 | loss 3.1739\n",
      "step 37500 | loss 2.8570\n",
      "step 37600 | loss 2.8738\n",
      "step 37700 | loss 2.2847\n",
      "step 37800 | loss 3.1666\n",
      "step 37900 | loss 2.8564\n",
      "step 38000 | loss 1.7004\n",
      "step 38100 | loss 2.3847\n",
      "step 38200 | loss 2.3594\n",
      "step 38300 | loss 2.9590\n",
      "step 38400 | loss 2.9153\n",
      "step 38500 | loss 1.7484\n",
      "step 38600 | loss 2.2628\n",
      "step 38700 | loss 3.2523\n",
      "step 38800 | loss 2.8991\n",
      "step 38900 | loss 2.8368\n",
      "step 39000 | loss 2.2248\n",
      "step 39100 | loss 2.6294\n",
      "step 39200 | loss 2.3140\n",
      "step 39300 | loss 2.5024\n",
      "step 39400 | loss 2.2475\n",
      "step 39500 | loss 2.4139\n",
      "step 39600 | loss 3.1127\n",
      "step 39700 | loss 2.7030\n",
      "step 39800 | loss 2.7037\n",
      "step 39900 | loss 3.0921\n",
      "step 40000 | loss 2.5887\n",
      "step 40100 | loss 3.2281\n",
      "step 40200 | loss 1.3585\n",
      "step 40300 | loss 3.0341\n",
      "step 40400 | loss 2.6541\n",
      "step 40500 | loss 3.1463\n",
      "step 40600 | loss 2.2736\n",
      "step 40700 | loss 2.2814\n",
      "step 40800 | loss 1.9162\n",
      "step 40900 | loss 2.3630\n",
      "step 41000 | loss 2.3916\n",
      "step 41100 | loss 2.0739\n",
      "step 41200 | loss 2.5671\n",
      "step 41300 | loss 2.5725\n",
      "step 41400 | loss 3.4832\n",
      "step 41500 | loss 2.2839\n",
      "step 41600 | loss 1.9959\n",
      "step 41700 | loss 2.9393\n",
      "step 41800 | loss 2.3382\n",
      "step 41900 | loss 2.7804\n",
      "step 42000 | loss 1.9705\n",
      "step 42100 | loss 1.8347\n",
      "step 42200 | loss 3.5191\n",
      "step 42300 | loss 2.8921\n",
      "step 42400 | loss 2.1552\n",
      "step 42500 | loss 2.7038\n",
      "step 42600 | loss 2.1561\n",
      "step 42700 | loss 2.9992\n",
      "step 42800 | loss 2.7387\n",
      "step 42900 | loss 3.1003\n",
      "step 43000 | loss 1.9733\n",
      "step 43100 | loss 3.6020\n",
      "step 43200 | loss 2.7803\n",
      "step 43300 | loss 3.6196\n",
      "step 43400 | loss 2.5207\n",
      "step 43500 | loss 2.9751\n",
      "step 43600 | loss 2.9285\n",
      "step 43700 | loss 1.8495\n",
      "step 43800 | loss 3.2930\n",
      "step 43900 | loss 1.7702\n",
      "step 44000 | loss 2.7172\n",
      "step 44100 | loss 1.4655\n",
      "step 44200 | loss 2.5714\n",
      "step 44300 | loss 2.9454\n",
      "step 44400 | loss 3.5007\n",
      "step 44500 | loss 2.5556\n",
      "step 44600 | loss 3.2610\n",
      "step 44700 | loss 2.2622\n",
      "step 44800 | loss 3.2364\n",
      "step 44900 | loss 2.5488\n",
      "step 45000 | loss 3.4787\n",
      "step 45100 | loss 2.9549\n",
      "step 45200 | loss 4.0534\n",
      "step 45300 | loss 2.3138\n",
      "step 45400 | loss 2.9119\n",
      "step 45500 | loss 2.8491\n",
      "step 45600 | loss 2.0663\n",
      "step 45700 | loss 2.0214\n",
      "step 45800 | loss 2.5540\n",
      "step 45900 | loss 2.8403\n",
      "step 46000 | loss 2.1862\n",
      "step 46100 | loss 2.2906\n",
      "step 46200 | loss 1.9269\n",
      "step 46300 | loss 2.7552\n",
      "step 46400 | loss 2.2916\n",
      "step 46500 | loss 2.4573\n",
      "step 46600 | loss 1.8612\n",
      "step 46700 | loss 1.9094\n",
      "step 46800 | loss 2.9096\n",
      "step 46900 | loss 2.0402\n",
      "step 47000 | loss 2.2717\n",
      "step 47100 | loss 2.8889\n",
      "step 47200 | loss 2.3454\n",
      "step 47300 | loss 2.3373\n",
      "step 47400 | loss 1.9543\n",
      "step 47500 | loss 3.2262\n",
      "step 47600 | loss 3.8306\n",
      "step 47700 | loss 3.7089\n",
      "step 47800 | loss 2.1563\n",
      "step 47900 | loss 1.7698\n",
      "step 48000 | loss 3.6785\n",
      "step 48100 | loss 2.7543\n",
      "step 48200 | loss 2.8155\n",
      "step 48300 | loss 3.0925\n",
      "step 48400 | loss 2.8891\n",
      "step 48500 | loss 2.1864\n",
      "step 48600 | loss 2.9030\n",
      "step 48700 | loss 3.3716\n",
      "step 48800 | loss 1.5469\n",
      "step 48900 | loss 3.3390\n",
      "step 49000 | loss 3.1063\n",
      "step 49100 | loss 3.2688\n",
      "step 49200 | loss 1.6781\n",
      "step 49300 | loss 2.3411\n",
      "step 49400 | loss 2.4428\n",
      "step 49500 | loss 3.2116\n",
      "step 49600 | loss 3.5498\n",
      "step 49700 | loss 2.4646\n",
      "step 49800 | loss 2.8847\n",
      "step 49900 | loss 2.0232\n",
      "step 50000 | loss 2.5088\n",
      "step 50100 | loss 2.6227\n",
      "step 50200 | loss 1.9886\n",
      "step 50300 | loss 2.0664\n",
      "step 50400 | loss 2.5009\n",
      "step 50500 | loss 2.6629\n",
      "step 50600 | loss 2.4048\n",
      "step 50700 | loss 3.4698\n",
      "step 50800 | loss 3.4557\n",
      "step 50900 | loss 2.5564\n",
      "step 51000 | loss 2.5376\n",
      "step 51100 | loss 2.4538\n",
      "step 51200 | loss 2.4946\n",
      "step 51300 | loss 2.2225\n",
      "step 51400 | loss 2.1046\n",
      "step 51500 | loss 2.6442\n",
      "step 51600 | loss 3.8981\n",
      "step 51700 | loss 1.9751\n",
      "step 51800 | loss 3.2938\n",
      "step 51900 | loss 1.8613\n",
      "step 52000 | loss 1.6507\n",
      "step 52100 | loss 2.3056\n",
      "step 52200 | loss 3.5085\n",
      "step 52300 | loss 2.2075\n",
      "step 52400 | loss 2.5512\n",
      "step 52500 | loss 2.2730\n",
      "step 52600 | loss 1.6726\n",
      "step 52700 | loss 3.6975\n",
      "step 52800 | loss 2.2022\n",
      "step 52900 | loss 2.1825\n",
      "step 53000 | loss 2.9835\n",
      "step 53100 | loss 3.3360\n",
      "step 53200 | loss 2.6601\n",
      "step 53300 | loss 3.3689\n",
      "step 53400 | loss 2.7692\n",
      "step 53500 | loss 2.1959\n",
      "step 53600 | loss 1.7697\n",
      "step 53700 | loss 2.2755\n",
      "step 53800 | loss 1.6984\n",
      "step 53900 | loss 3.2469\n",
      "step 54000 | loss 3.3353\n",
      "step 54100 | loss 1.8327\n",
      "step 54200 | loss 1.6818\n",
      "step 54300 | loss 2.4569\n",
      "step 54400 | loss 2.5458\n",
      "step 54500 | loss 3.6397\n",
      "step 54600 | loss 2.9398\n",
      "step 54700 | loss 2.1582\n",
      "step 54800 | loss 3.0621\n",
      "step 54900 | loss 2.1918\n",
      "step 55000 | loss 2.8475\n",
      "step 55100 | loss 3.1172\n",
      "step 55200 | loss 2.3102\n",
      "step 55300 | loss 3.3909\n",
      "step 55400 | loss 2.4738\n",
      "step 55500 | loss 3.3811\n",
      "step 55600 | loss 2.5728\n",
      "step 55700 | loss 1.8803\n",
      "step 55800 | loss 2.6495\n",
      "step 55900 | loss 1.8309\n",
      "step 56000 | loss 1.4278\n",
      "step 56100 | loss 2.0656\n",
      "step 56200 | loss 2.3420\n",
      "step 56300 | loss 3.3887\n",
      "step 56400 | loss 1.6614\n",
      "step 56500 | loss 2.0498\n",
      "step 56600 | loss 2.6436\n",
      "step 56700 | loss 3.5946\n",
      "step 56800 | loss 2.2314\n",
      "step 56900 | loss 2.2880\n",
      "step 57000 | loss 1.8599\n",
      "step 57100 | loss 3.7483\n",
      "step 57200 | loss 2.0287\n",
      "step 57300 | loss 3.2859\n",
      "step 57400 | loss 3.4200\n",
      "step 57500 | loss 2.2780\n",
      "step 57600 | loss 2.7280\n",
      "step 57700 | loss 2.6516\n",
      "step 57800 | loss 2.8899\n",
      "step 57900 | loss 2.4578\n",
      "step 58000 | loss 1.6410\n",
      "step 58100 | loss 1.8527\n",
      "step 58200 | loss 2.9463\n",
      "step 58300 | loss 2.7845\n",
      "step 58400 | loss 1.6256\n",
      "step 58500 | loss 2.8997\n",
      "step 58600 | loss 2.9700\n",
      "step 58700 | loss 3.3747\n",
      "step 58800 | loss 2.4490\n",
      "step 58900 | loss 2.4129\n",
      "step 59000 | loss 1.9704\n",
      "step 59100 | loss 2.2774\n",
      "step 59200 | loss 2.7214\n",
      "step 59300 | loss 3.7119\n",
      "step 59400 | loss 2.4416\n",
      "step 59500 | loss 3.3077\n",
      "step 59600 | loss 2.0648\n",
      "step 59700 | loss 2.3094\n",
      "step 59800 | loss 2.7665\n",
      "step 59900 | loss 2.3636\n",
      "step 60000 | loss 2.5494\n",
      "step 60100 | loss 2.4134\n",
      "step 60200 | loss 2.6824\n",
      "step 60300 | loss 2.3135\n",
      "step 60400 | loss 2.8354\n",
      "step 60500 | loss 3.4371\n",
      "step 60600 | loss 2.2849\n",
      "step 60700 | loss 2.0571\n",
      "step 60800 | loss 2.1441\n",
      "step 60900 | loss 1.8450\n",
      "step 61000 | loss 2.8437\n",
      "step 61100 | loss 3.0051\n",
      "step 61200 | loss 1.9539\n",
      "step 61300 | loss 1.8979\n",
      "step 61400 | loss 2.9467\n",
      "step 61500 | loss 2.9118\n",
      "step 61600 | loss 1.7592\n",
      "step 61700 | loss 3.5388\n",
      "step 61800 | loss 2.6423\n",
      "step 61900 | loss 3.7612\n",
      "step 62000 | loss 1.9239\n",
      "step 62100 | loss 2.6392\n",
      "step 62200 | loss 3.2332\n",
      "step 62300 | loss 2.8470\n",
      "step 62400 | loss 2.7031\n",
      "step 62500 | loss 2.6124\n",
      "step 62600 | loss 1.8302\n",
      "step 62700 | loss 2.0538\n",
      "step 62800 | loss 2.2437\n",
      "step 62900 | loss 1.7965\n",
      "step 63000 | loss 1.7546\n",
      "step 63100 | loss 2.2075\n",
      "step 63200 | loss 2.5908\n",
      "step 63300 | loss 2.4223\n",
      "step 63400 | loss 2.7328\n",
      "step 63500 | loss 3.6201\n",
      "step 63600 | loss 3.1219\n",
      "step 63700 | loss 3.6317\n",
      "step 63800 | loss 2.9630\n",
      "step 63900 | loss 2.1858\n",
      "step 64000 | loss 2.3169\n",
      "step 64100 | loss 2.6014\n",
      "step 64200 | loss 1.9543\n",
      "step 64300 | loss 3.3953\n",
      "step 64400 | loss 1.2664\n",
      "step 64500 | loss 2.9464\n",
      "step 64600 | loss 2.9571\n",
      "step 64700 | loss 4.0022\n",
      "step 64800 | loss 2.8438\n",
      "step 64900 | loss 2.1035\n",
      "step 65000 | loss 3.2968\n",
      "step 65100 | loss 2.7931\n",
      "step 65200 | loss 1.3626\n",
      "step 65300 | loss 2.2024\n",
      "step 65400 | loss 3.0829\n",
      "step 65500 | loss 1.9829\n",
      "step 65600 | loss 2.6199\n",
      "step 65700 | loss 2.3950\n",
      "step 65800 | loss 3.1541\n",
      "step 65900 | loss 2.3226\n",
      "step 66000 | loss 2.1990\n",
      "step 66100 | loss 2.7404\n",
      "step 66200 | loss 2.9106\n",
      "step 66300 | loss 2.0095\n",
      "step 66400 | loss 3.3068\n",
      "step 66500 | loss 2.7473\n",
      "step 66600 | loss 2.7266\n",
      "step 66700 | loss 1.6421\n",
      "step 66800 | loss 1.9803\n",
      "step 66900 | loss 1.6216\n",
      "step 67000 | loss 2.7583\n",
      "step 67100 | loss 2.6655\n",
      "step 67200 | loss 1.2546\n",
      "step 67300 | loss 1.2215\n",
      "step 67400 | loss 2.3625\n",
      "step 67500 | loss 2.2188\n",
      "step 67600 | loss 2.2099\n",
      "step 67700 | loss 1.9190\n",
      "step 67800 | loss 3.0499\n",
      "step 67900 | loss 2.1135\n",
      "step 68000 | loss 2.6991\n",
      "step 68100 | loss 2.2600\n",
      "step 68200 | loss 2.8258\n",
      "step 68300 | loss 2.8837\n",
      "step 68400 | loss 2.2207\n",
      "step 68500 | loss 2.8277\n",
      "step 68600 | loss 1.9149\n",
      "step 68700 | loss 3.2261\n",
      "step 68800 | loss 2.7626\n",
      "step 68900 | loss 3.4587\n",
      "step 69000 | loss 2.4699\n",
      "step 69100 | loss 2.9114\n",
      "step 69200 | loss 3.0858\n",
      "step 69300 | loss 1.2270\n",
      "step 69400 | loss 2.6269\n",
      "step 69500 | loss 2.5867\n",
      "step 69600 | loss 3.1840\n",
      "step 69700 | loss 2.6380\n",
      "step 69800 | loss 1.9205\n",
      "step 69900 | loss 2.3761\n",
      "step 70000 | loss 2.9194\n",
      "step 70100 | loss 1.2597\n",
      "step 70200 | loss 3.1573\n",
      "step 70300 | loss 2.7367\n",
      "step 70400 | loss 2.5785\n",
      "step 70500 | loss 2.0897\n",
      "step 70600 | loss 2.3862\n",
      "step 70700 | loss 1.2561\n",
      "step 70800 | loss 1.7571\n",
      "step 70900 | loss 1.9126\n",
      "step 71000 | loss 3.4465\n",
      "step 71100 | loss 2.1705\n",
      "step 71200 | loss 1.7541\n",
      "step 71300 | loss 2.3121\n",
      "step 71400 | loss 2.4576\n",
      "step 71500 | loss 1.6118\n",
      "step 71600 | loss 2.1181\n",
      "step 71700 | loss 2.5708\n",
      "step 71800 | loss 2.3331\n",
      "step 71900 | loss 3.5195\n",
      "step 72000 | loss 1.8365\n",
      "step 72100 | loss 2.1335\n",
      "step 72200 | loss 2.0610\n",
      "step 72300 | loss 2.7279\n",
      "step 72400 | loss 2.8539\n",
      "step 72500 | loss 2.7696\n",
      "step 72600 | loss 2.4288\n",
      "step 72700 | loss 3.6905\n",
      "step 72800 | loss 2.7198\n",
      "step 72900 | loss 3.4606\n",
      "step 73000 | loss 2.4569\n",
      "step 73100 | loss 3.6195\n",
      "step 73200 | loss 3.6814\n",
      "step 73300 | loss 1.8876\n",
      "step 73400 | loss 1.1303\n",
      "step 73500 | loss 2.1785\n",
      "step 73600 | loss 2.9635\n",
      "step 73700 | loss 1.7072\n",
      "step 73800 | loss 3.4105\n",
      "step 73900 | loss 2.8192\n",
      "step 74000 | loss 2.8120\n",
      "step 74100 | loss 2.9858\n",
      "step 74200 | loss 1.9358\n",
      "step 74300 | loss 3.0885\n",
      "step 74400 | loss 2.5208\n",
      "step 74500 | loss 2.2377\n",
      "step 74600 | loss 1.8328\n",
      "step 74700 | loss 2.6745\n",
      "step 74800 | loss 2.3413\n",
      "step 74900 | loss 2.7572\n",
      "step 75000 | loss 2.0592\n",
      "step 75100 | loss 1.9345\n",
      "step 75200 | loss 3.7709\n",
      "step 75300 | loss 2.4807\n",
      "step 75400 | loss 3.2791\n",
      "step 75500 | loss 2.5746\n",
      "step 75600 | loss 1.6519\n",
      "step 75700 | loss 2.0923\n",
      "step 75800 | loss 1.9744\n",
      "step 75900 | loss 2.4816\n",
      "step 76000 | loss 2.0012\n",
      "step 76100 | loss 1.8996\n",
      "step 76200 | loss 2.8175\n",
      "step 76300 | loss 2.9752\n",
      "step 76400 | loss 1.8812\n",
      "step 76500 | loss 1.8292\n",
      "step 76600 | loss 2.5010\n",
      "step 76700 | loss 2.6410\n",
      "step 76800 | loss 2.3286\n",
      "step 76900 | loss 3.3371\n",
      "step 77000 | loss 2.8926\n",
      "step 77100 | loss 2.1553\n",
      "step 77200 | loss 2.4672\n",
      "step 77300 | loss 1.7190\n",
      "step 77400 | loss 3.1983\n",
      "step 77500 | loss 1.9102\n",
      "step 77600 | loss 2.0653\n",
      "step 77700 | loss 2.5871\n",
      "step 77800 | loss 2.6428\n",
      "step 77900 | loss 2.6141\n",
      "step 78000 | loss 3.0703\n",
      "step 78100 | loss 1.9195\n",
      "step 78200 | loss 2.3775\n",
      "step 78300 | loss 2.4177\n",
      "step 78400 | loss 2.6765\n",
      "step 78500 | loss 2.4906\n",
      "step 78600 | loss 2.9214\n",
      "step 78700 | loss 1.8713\n",
      "step 78800 | loss 3.1951\n",
      "step 78900 | loss 2.2428\n",
      "step 79000 | loss 2.4626\n",
      "step 79100 | loss 2.4756\n",
      "step 79200 | loss 2.3104\n",
      "step 79300 | loss 1.8443\n",
      "step 79400 | loss 2.6518\n",
      "step 79500 | loss 3.1937\n",
      "step 79600 | loss 1.9821\n",
      "step 79700 | loss 3.4842\n",
      "step 79800 | loss 2.5188\n",
      "step 79900 | loss 2.8614\n",
      "step 80000 | loss 2.2611\n",
      "step 80100 | loss 3.1157\n",
      "step 80200 | loss 2.2553\n",
      "step 80300 | loss 2.3796\n",
      "step 80400 | loss 2.6486\n",
      "step 80500 | loss 1.7401\n",
      "step 80600 | loss 1.5666\n",
      "step 80700 | loss 3.0134\n",
      "step 80800 | loss 2.2154\n",
      "step 80900 | loss 2.8953\n",
      "step 81000 | loss 2.0936\n",
      "step 81100 | loss 2.3514\n",
      "step 81200 | loss 2.8166\n",
      "step 81300 | loss 2.5067\n",
      "step 81400 | loss 2.0915\n",
      "step 81500 | loss 2.2833\n",
      "step 81600 | loss 1.9627\n",
      "step 81700 | loss 2.4993\n",
      "step 81800 | loss 3.2112\n",
      "step 81900 | loss 2.2803\n",
      "step 82000 | loss 2.3166\n",
      "step 82100 | loss 1.4607\n",
      "step 82200 | loss 2.5600\n",
      "step 82300 | loss 2.1709\n",
      "step 82400 | loss 3.2389\n",
      "step 82500 | loss 1.7529\n",
      "step 82600 | loss 2.0870\n",
      "step 82700 | loss 2.6471\n",
      "step 82800 | loss 2.7840\n",
      "step 82900 | loss 2.3411\n",
      "step 83000 | loss 2.8956\n",
      "step 83100 | loss 1.6071\n",
      "step 83200 | loss 3.2818\n",
      "step 83300 | loss 2.0025\n",
      "step 83400 | loss 2.3502\n",
      "step 83500 | loss 1.9670\n",
      "step 83600 | loss 2.3351\n",
      "step 83700 | loss 2.5407\n",
      "step 83800 | loss 2.7073\n",
      "step 83900 | loss 2.4483\n",
      "step 84000 | loss 1.7947\n",
      "step 84100 | loss 1.9828\n",
      "step 84200 | loss 2.9410\n",
      "step 84300 | loss 2.0003\n",
      "step 84400 | loss 2.0669\n",
      "step 84500 | loss 2.0816\n",
      "step 84600 | loss 2.8190\n",
      "step 84700 | loss 2.7332\n",
      "step 84800 | loss 2.4942\n",
      "step 84900 | loss 3.3273\n",
      "step 85000 | loss 2.4698\n",
      "step 85100 | loss 2.4388\n",
      "step 85200 | loss 2.3012\n",
      "step 85300 | loss 3.3621\n",
      "step 85400 | loss 2.6053\n",
      "step 85500 | loss 3.6055\n",
      "step 85600 | loss 1.3505\n",
      "step 85700 | loss 3.6081\n",
      "step 85800 | loss 2.0703\n",
      "step 85900 | loss 2.4523\n",
      "step 86000 | loss 2.3316\n",
      "step 86100 | loss 2.1436\n",
      "step 86200 | loss 1.8718\n",
      "step 86300 | loss 2.2610\n",
      "step 86400 | loss 3.7835\n",
      "step 86500 | loss 3.1326\n",
      "step 86600 | loss 2.2007\n",
      "step 86700 | loss 1.9533\n",
      "step 86800 | loss 2.8592\n",
      "step 86900 | loss 2.3582\n",
      "step 87000 | loss 1.5584\n",
      "step 87100 | loss 1.8659\n",
      "step 87200 | loss 2.6968\n",
      "step 87300 | loss 2.8296\n",
      "step 87400 | loss 2.4905\n",
      "step 87500 | loss 1.9842\n",
      "step 87600 | loss 2.1955\n",
      "step 87700 | loss 2.3664\n",
      "step 87800 | loss 2.3423\n",
      "step 87900 | loss 3.2318\n",
      "step 88000 | loss 3.8287\n",
      "step 88100 | loss 2.1942\n",
      "step 88200 | loss 2.5654\n",
      "step 88300 | loss 1.7500\n",
      "step 88400 | loss 2.5174\n",
      "step 88500 | loss 1.8221\n",
      "step 88600 | loss 1.6479\n",
      "step 88700 | loss 2.5434\n",
      "step 88800 | loss 3.3216\n",
      "step 88900 | loss 3.2927\n",
      "step 89000 | loss 1.9865\n",
      "step 89100 | loss 2.6136\n",
      "step 89200 | loss 2.7844\n",
      "step 89300 | loss 2.3053\n",
      "step 89400 | loss 2.4287\n",
      "step 89500 | loss 2.8559\n",
      "step 89600 | loss 3.2956\n",
      "step 89700 | loss 2.5111\n",
      "step 89800 | loss 1.8161\n",
      "step 89900 | loss 2.9329\n",
      "step 90000 | loss 1.2103\n",
      "step 90100 | loss 1.1260\n",
      "step 90200 | loss 3.1930\n",
      "step 90300 | loss 2.1656\n",
      "step 90400 | loss 2.6805\n",
      "step 90500 | loss 2.5659\n",
      "step 90600 | loss 3.5343\n",
      "step 90700 | loss 3.4342\n",
      "step 90800 | loss 2.8307\n",
      "step 90900 | loss 2.8787\n",
      "step 91000 | loss 1.7259\n",
      "step 91100 | loss 2.4279\n",
      "step 91200 | loss 3.0507\n",
      "step 91300 | loss 1.6343\n",
      "step 91400 | loss 1.7424\n",
      "step 91500 | loss 3.3258\n",
      "step 91600 | loss 2.6839\n",
      "step 91700 | loss 2.5093\n",
      "step 91800 | loss 1.9523\n",
      "step 91900 | loss 2.6136\n",
      "step 92000 | loss 1.7349\n",
      "step 92100 | loss 2.7815\n",
      "step 92200 | loss 1.9559\n",
      "step 92300 | loss 1.7219\n",
      "step 92400 | loss 1.9057\n",
      "step 92500 | loss 3.0619\n",
      "step 92600 | loss 2.2813\n",
      "step 92700 | loss 2.9226\n",
      "step 92800 | loss 3.0678\n",
      "step 92900 | loss 1.2000\n",
      "step 93000 | loss 2.6097\n",
      "step 93100 | loss 2.4592\n",
      "step 93200 | loss 3.3045\n",
      "step 93300 | loss 2.8091\n",
      "step 93400 | loss 3.4354\n",
      "step 93500 | loss 1.9397\n",
      "step 93600 | loss 2.0184\n",
      "step 93700 | loss 1.3110\n",
      "step 93800 | loss 2.6937\n",
      "step 93900 | loss 3.5924\n",
      "step 94000 | loss 2.3632\n",
      "step 94100 | loss 2.8147\n",
      "step 94200 | loss 2.0247\n",
      "step 94300 | loss 2.5723\n",
      "step 94400 | loss 1.9976\n",
      "step 94500 | loss 2.5573\n",
      "step 94600 | loss 2.0742\n",
      "step 94700 | loss 2.5692\n",
      "step 94800 | loss 1.2303\n",
      "step 94900 | loss 2.5128\n",
      "step 95000 | loss 2.4959\n",
      "step 95100 | loss 2.5370\n",
      "step 95200 | loss 2.0768\n",
      "step 95300 | loss 2.8441\n",
      "step 95400 | loss 2.8082\n",
      "step 95500 | loss 1.8876\n",
      "step 95600 | loss 1.6702\n",
      "step 95700 | loss 3.0698\n",
      "step 95800 | loss 2.6521\n",
      "step 95900 | loss 3.1829\n",
      "step 96000 | loss 3.0463\n",
      "step 96100 | loss 3.5776\n",
      "step 96200 | loss 2.7228\n",
      "step 96300 | loss 3.1353\n",
      "step 96400 | loss 1.8808\n",
      "step 96500 | loss 3.4414\n",
      "step 96600 | loss 2.3283\n",
      "step 96700 | loss 3.1071\n",
      "step 96800 | loss 2.7557\n",
      "step 96900 | loss 2.3976\n",
      "step 97000 | loss 2.5379\n",
      "step 97100 | loss 2.2868\n",
      "step 97200 | loss 1.7963\n",
      "step 97300 | loss 3.0264\n",
      "step 97400 | loss 1.9857\n",
      "step 97500 | loss 2.2287\n",
      "step 97600 | loss 2.1450\n",
      "step 97700 | loss 2.3374\n",
      "step 97800 | loss 1.7750\n",
      "step 97900 | loss 3.0479\n",
      "step 98000 | loss 3.1974\n",
      "step 98100 | loss 2.5172\n",
      "step 98200 | loss 2.3026\n",
      "step 98300 | loss 1.9465\n",
      "step 98400 | loss 2.0644\n",
      "step 98500 | loss 2.9215\n",
      "step 98600 | loss 1.9110\n",
      "step 98700 | loss 2.4158\n",
      "step 98800 | loss 2.5254\n",
      "step 98900 | loss 1.9747\n",
      "step 99000 | loss 1.8504\n",
      "step 99100 | loss 2.3910\n",
      "step 99200 | loss 2.0626\n",
      "step 99300 | loss 1.9519\n",
      "step 99400 | loss 2.6748\n",
      "step 99500 | loss 1.5828\n",
      "step 99600 | loss 2.2547\n",
      "step 99700 | loss 2.3814\n",
      "step 99800 | loss 1.9179\n",
      "step 99900 | loss 1.9617\n",
      "step 100000 | loss 2.4067\n",
      "step 100100 | loss 2.1855\n",
      "step 100200 | loss 2.2408\n",
      "step 100300 | loss 3.1452\n",
      "step 100400 | loss 3.1401\n",
      "step 100500 | loss 2.8808\n",
      "step 100600 | loss 2.6824\n",
      "step 100700 | loss 2.8486\n",
      "step 100800 | loss 3.1749\n",
      "step 100900 | loss 2.4438\n",
      "step 101000 | loss 2.0738\n",
      "step 101100 | loss 2.7487\n",
      "step 101200 | loss 2.2902\n",
      "step 101300 | loss 2.1837\n",
      "step 101400 | loss 2.7045\n",
      "step 101500 | loss 2.0631\n",
      "step 101600 | loss 2.5424\n",
      "step 101700 | loss 3.4579\n",
      "step 101800 | loss 2.3729\n",
      "step 101900 | loss 1.9720\n",
      "step 102000 | loss 1.9146\n",
      "step 102100 | loss 2.3453\n",
      "step 102200 | loss 2.3808\n",
      "step 102300 | loss 2.6276\n",
      "step 102400 | loss 2.7674\n",
      "step 102500 | loss 3.4397\n",
      "step 102600 | loss 3.3282\n",
      "step 102700 | loss 2.4923\n",
      "step 102800 | loss 3.4961\n",
      "step 102900 | loss 3.6624\n",
      "step 103000 | loss 1.2549\n",
      "step 103100 | loss 3.5375\n",
      "step 103200 | loss 3.2154\n",
      "step 103300 | loss 1.1539\n",
      "step 103400 | loss 2.0279\n",
      "step 103500 | loss 3.2323\n",
      "step 103600 | loss 2.0712\n",
      "step 103700 | loss 2.6126\n",
      "step 103800 | loss 1.8076\n",
      "step 103900 | loss 1.7406\n",
      "step 104000 | loss 2.6504\n",
      "step 104100 | loss 2.0706\n",
      "step 104200 | loss 3.3494\n",
      "step 104300 | loss 2.0303\n",
      "step 104400 | loss 1.9173\n",
      "step 104500 | loss 2.1875\n",
      "step 104600 | loss 3.5870\n",
      "step 104700 | loss 3.4274\n",
      "step 104800 | loss 2.1511\n",
      "step 104900 | loss 2.2597\n",
      "step 105000 | loss 2.7689\n",
      "step 105100 | loss 2.7728\n",
      "step 105200 | loss 2.1369\n",
      "step 105300 | loss 2.3638\n",
      "step 105400 | loss 2.9603\n",
      "step 105500 | loss 1.9384\n",
      "step 105600 | loss 2.4955\n",
      "step 105700 | loss 2.5219\n",
      "step 105800 | loss 1.6123\n",
      "step 105900 | loss 2.8071\n",
      "step 106000 | loss 2.2296\n",
      "step 106100 | loss 1.4081\n",
      "step 106200 | loss 1.8591\n",
      "step 106300 | loss 2.0331\n",
      "step 106400 | loss 2.2822\n",
      "step 106500 | loss 2.4319\n",
      "step 106600 | loss 1.6757\n",
      "step 106700 | loss 1.8597\n",
      "step 106800 | loss 2.4672\n",
      "step 106900 | loss 1.6998\n",
      "step 107000 | loss 2.6455\n",
      "step 107100 | loss 2.7652\n",
      "step 107200 | loss 2.2496\n",
      "step 107300 | loss 1.6923\n",
      "step 107400 | loss 1.9852\n",
      "step 107500 | loss 3.0581\n",
      "step 107600 | loss 2.4240\n",
      "step 107700 | loss 3.1041\n",
      "step 107800 | loss 2.8713\n",
      "step 107900 | loss 3.0115\n",
      "step 108000 | loss 2.4417\n",
      "step 108100 | loss 2.3774\n",
      "step 108200 | loss 3.3584\n",
      "step 108300 | loss 2.6591\n",
      "step 108400 | loss 1.9667\n",
      "step 108500 | loss 2.9951\n",
      "step 108600 | loss 3.1553\n",
      "step 108700 | loss 2.6584\n",
      "step 108800 | loss 2.2589\n",
      "step 108900 | loss 2.5189\n",
      "step 109000 | loss 3.2255\n",
      "step 109100 | loss 1.9739\n",
      "step 109200 | loss 0.9528\n",
      "step 109300 | loss 2.8543\n",
      "step 109400 | loss 2.2963\n",
      "step 109500 | loss 2.9440\n",
      "step 109600 | loss 3.0740\n",
      "step 109700 | loss 1.4663\n",
      "step 109800 | loss 1.7248\n",
      "step 109900 | loss 2.5625\n",
      "step 110000 | loss 2.2380\n",
      "step 110100 | loss 2.6858\n",
      "step 110200 | loss 2.4750\n",
      "step 110300 | loss 2.3589\n",
      "step 110400 | loss 2.4349\n",
      "step 110500 | loss 3.0093\n",
      "step 110600 | loss 3.0021\n",
      "step 110700 | loss 3.1928\n",
      "step 110800 | loss 1.3480\n",
      "step 110900 | loss 3.0145\n",
      "step 111000 | loss 1.9642\n",
      "step 111100 | loss 2.3241\n",
      "step 111200 | loss 1.9628\n",
      "step 111300 | loss 2.9367\n",
      "step 111400 | loss 2.1793\n",
      "step 111500 | loss 1.8257\n",
      "step 111600 | loss 2.1972\n",
      "step 111700 | loss 2.0672\n",
      "step 111800 | loss 3.2192\n",
      "step 111900 | loss 2.2427\n",
      "step 112000 | loss 3.3747\n",
      "step 112100 | loss 2.7036\n",
      "step 112200 | loss 2.5662\n",
      "step 112300 | loss 2.6627\n",
      "step 112400 | loss 2.6467\n",
      "step 112500 | loss 3.2712\n",
      "step 112600 | loss 2.3759\n",
      "step 112700 | loss 1.9677\n",
      "step 112800 | loss 1.5541\n",
      "step 112900 | loss 1.7096\n",
      "step 113000 | loss 1.3436\n",
      "step 113100 | loss 2.2136\n",
      "step 113200 | loss 3.2274\n",
      "step 113300 | loss 2.3346\n",
      "step 113400 | loss 2.4503\n",
      "step 113500 | loss 3.4328\n",
      "step 113600 | loss 2.1711\n",
      "step 113700 | loss 2.3759\n",
      "step 113800 | loss 2.7060\n",
      "step 113900 | loss 2.7944\n",
      "step 114000 | loss 1.5118\n",
      "step 114100 | loss 2.3815\n",
      "step 114200 | loss 1.9723\n",
      "step 114300 | loss 1.9680\n",
      "step 114400 | loss 2.9460\n",
      "step 114500 | loss 1.8516\n",
      "step 114600 | loss 2.5539\n",
      "step 114700 | loss 2.5351\n",
      "step 114800 | loss 2.0027\n",
      "step 114900 | loss 2.5584\n",
      "step 115000 | loss 2.4509\n",
      "step 115100 | loss 1.2625\n",
      "step 115200 | loss 2.6869\n",
      "step 115300 | loss 3.3511\n",
      "step 115400 | loss 3.4768\n",
      "step 115500 | loss 2.5222\n",
      "step 115600 | loss 1.8345\n",
      "step 115700 | loss 2.5559\n",
      "step 115800 | loss 2.0325\n",
      "step 115900 | loss 2.6131\n",
      "step 116000 | loss 2.0662\n",
      "step 116100 | loss 3.6334\n",
      "step 116200 | loss 3.4364\n",
      "step 116300 | loss 1.8805\n",
      "step 116400 | loss 2.6522\n",
      "step 116500 | loss 2.0337\n",
      "step 116600 | loss 1.7337\n",
      "step 116700 | loss 2.1159\n",
      "step 116800 | loss 3.0664\n",
      "step 116900 | loss 2.5676\n",
      "step 117000 | loss 2.1609\n",
      "step 117100 | loss 1.9507\n",
      "step 117200 | loss 2.7234\n",
      "step 117300 | loss 3.1357\n",
      "step 117400 | loss 2.6579\n",
      "step 117500 | loss 2.7441\n",
      "step 117600 | loss 3.5823\n",
      "step 117700 | loss 1.5453\n",
      "step 117800 | loss 3.0460\n",
      "step 117900 | loss 3.0407\n",
      "step 118000 | loss 3.0132\n",
      "step 118100 | loss 3.5165\n",
      "step 118200 | loss 2.4261\n",
      "step 118300 | loss 2.1015\n",
      "step 118400 | loss 3.0308\n",
      "step 118500 | loss 1.5960\n",
      "step 118600 | loss 2.2821\n",
      "step 118700 | loss 2.3304\n",
      "step 118800 | loss 3.4277\n",
      "step 118900 | loss 3.2645\n",
      "step 119000 | loss 1.8490\n",
      "step 119100 | loss 1.7369\n",
      "step 119200 | loss 2.8533\n",
      "step 119300 | loss 1.5869\n",
      "step 119400 | loss 2.8596\n",
      "step 119500 | loss 1.9899\n",
      "step 119600 | loss 2.6394\n",
      "step 119700 | loss 2.8218\n",
      "step 119800 | loss 2.3546\n",
      "step 119900 | loss 2.3004\n",
      "step 120000 | loss 2.6883\n",
      "step 120100 | loss 2.9320\n",
      "step 120200 | loss 2.1528\n",
      "step 120300 | loss 2.5363\n",
      "step 120400 | loss 2.1145\n",
      "step 120500 | loss 2.1528\n",
      "step 120600 | loss 2.3555\n",
      "step 120700 | loss 2.8189\n",
      "step 120800 | loss 3.5366\n",
      "step 120900 | loss 3.4545\n",
      "step 121000 | loss 3.6372\n",
      "step 121100 | loss 3.4166\n",
      "step 121200 | loss 2.4264\n",
      "step 121300 | loss 1.9622\n",
      "step 121400 | loss 2.5420\n",
      "step 121500 | loss 2.3271\n",
      "step 121600 | loss 2.5939\n",
      "step 121700 | loss 2.3462\n",
      "step 121800 | loss 1.9933\n",
      "step 121900 | loss 2.4245\n",
      "step 122000 | loss 1.7647\n",
      "step 122100 | loss 2.0321\n",
      "step 122200 | loss 3.3711\n",
      "step 122300 | loss 1.7124\n",
      "step 122400 | loss 3.0382\n",
      "step 122500 | loss 2.0858\n",
      "step 122600 | loss 1.9315\n",
      "step 122700 | loss 3.5403\n",
      "step 122800 | loss 1.5133\n",
      "step 122900 | loss 1.9222\n",
      "step 123000 | loss 2.9263\n",
      "step 123100 | loss 2.2067\n",
      "step 123200 | loss 1.7489\n",
      "step 123300 | loss 1.9552\n",
      "step 123400 | loss 3.4272\n",
      "step 123500 | loss 2.0516\n",
      "step 123600 | loss 1.9172\n",
      "step 123700 | loss 2.9251\n",
      "step 123800 | loss 1.1498\n",
      "step 123900 | loss 2.4580\n",
      "step 124000 | loss 2.1629\n",
      "step 124100 | loss 2.5239\n",
      "step 124200 | loss 1.4310\n",
      "step 124300 | loss 1.7262\n",
      "step 124400 | loss 1.4263\n",
      "step 124500 | loss 2.8342\n",
      "step 124600 | loss 2.0169\n",
      "step 124700 | loss 2.9153\n",
      "step 124800 | loss 2.3501\n",
      "step 124900 | loss 1.0483\n",
      "step 125000 | loss 2.3105\n",
      "step 125100 | loss 1.4142\n",
      "step 125200 | loss 2.7562\n",
      "step 125300 | loss 2.5298\n",
      "step 125400 | loss 2.1108\n",
      "step 125500 | loss 2.4170\n",
      "step 125600 | loss 3.4290\n",
      "step 125700 | loss 2.5293\n",
      "step 125800 | loss 2.6040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m logits \u001b[39m=\u001b[39m model(x, attn_mask)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size), y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m accumulation_step:\n\u001b[1;32m     22\u001b[0m     optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(tokenizer)\n",
    "model = MiniTransformerLM(vocab_size).to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "accumulation_step = 64 / 8\n",
    "\n",
    "optim.zero_grad()\n",
    "for step, (x, y) in enumerate(loader):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    B, T = x.shape\n",
    "\n",
    "    attn_mask = torch.triu(torch.ones(T, T, device=device) * float('-inf'), diagonal=1)\n",
    "\n",
    "    logits = model(x, attn_mask)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    if (step + 1) % accumulation_step:\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")\n",
    "        torch.save(model.state_dict(), 'llm.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a wallet on a flying to practice intends to travel alert when in Edinburgh. The taxi types are typically you can fly from Â£10 to take you to UK retail but there are extra baggage and entry you will insist will also begin your holidays with the route along.\n",
      "There\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assume 'tokenizer' is your tokenizer with encode/decode methods\n",
    "# vocab_size should match the tokenizer\n",
    "vocab_size = len(tokenizer)\n",
    "model = MiniTransformerLM(vocab_size)\n",
    "state_dict = torch.load(\"llm.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0):\n",
    "    # Encode prompt\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)  # shape: [1, T]\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # causal mask: prevent attending to future tokens\n",
    "        T = input_ids.shape[1]\n",
    "        mask = torch.tril(torch.ones(T, T, device=device)).unsqueeze(0)  # shape [1, T, T]\n",
    "        \n",
    "        # Get logits\n",
    "        logits = model(input_ids, mask=mask)  # shape: [1, T, vocab_size]\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "        \n",
    "        # Sample next token\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).unsqueeze(0)  # shape [1, 1]\n",
    "        \n",
    "        # Append to input_ids\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    # Decode tokens\n",
    "    return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "# Example usage\n",
    "prompt = \"There is a wallet on a\"\n",
    "generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=50)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
